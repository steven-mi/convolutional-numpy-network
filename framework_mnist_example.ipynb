{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML-Fundamentals - Neural Networks - Neural Network Framework Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Requirements](#Requirements) \n",
    "  * [Modules](#Python-Modules) \n",
    "  * [Data](#Data)\n",
    "* [Neural Network Framework Example](#Neural-Network-Framework-Example)\n",
    "  * [Network](#NerualNetwork-Class)\n",
    "  * [Layer](#Layers)\n",
    "  * [Activation Function](#Activation-Function)   \n",
    "  * [Loss Function](#Loss-Function)\n",
    "  * [Optimizer](#Optimization-with-SGD)\n",
    "  * [Creating a MNIST Neural Network](#Put-it-all-together)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python-Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third party\n",
    "import numpy as np\n",
    "from deep_teaching_commons.data.fundamentals.mnist import Mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto download is active, attempting download\n",
      "mnist data directory already exists, download aborted\n",
      "(60000, 28, 28) (60000,)\n",
      "(60000, 1, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "# create mnist loader from deep_teaching_commons\n",
    "mnist_loader = Mnist(data_dir='data')\n",
    "\n",
    "# load all data, labels are one-hot-encoded, images are flatten and pixel squashed between [0,1]\n",
    "train_images, train_labels, test_images, test_labels = mnist_loader.get_all_data(flatten=False, one_hot_enc=False, normalized=True)\n",
    "print(train_images.shape, train_labels.shape)\n",
    "\n",
    "# reshape to match generel framework architecture \n",
    "train_images, test_images = train_images.reshape(60000, 1, 28, 28), test_images.reshape(10000, 1, 28, 28)            \n",
    "print(train_images.shape, train_labels.shape)\n",
    "\n",
    "# shuffle training data\n",
    "shuffle_index = np.random.permutation(60000)\n",
    "train_images, train_labels = train_images[shuffle_index], train_labels[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Framework Example\n",
    "For a better understanding of neural networks you will start to implement your own framework. The given notebook explaines some core functions and concetps of the framework, so you all have the same starting point. Our previous exercises were self-contained and not very modular. You are going to change that. Let us begin with a fully connected network on the MNIST data set. The  Pipeline will be: \n",
    "\n",
    "**define a model architecture -> construct a neural network from the model -> define a evaluation citeria -> optimize the network**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a custom architecture\n",
    "To create a custom model you have to define layers and activation functions that can be used to do so. Layers and activation functions are modeled as objects. Each object that you want to use has to implement a `forward` and a `backward` method that is used by the `NeuralNetwork` class. Additionally the `self.params` attribute is mandatory to meet the specification of the `NeuralNetwork` class. It is used to store all learnable parameters that you need for the optimization algorithm. Implemented that way you can use the objects as building blocks and stack them up to create a custom model. Be aware of using an activation function after each layer except the last one, cause the softmax function is applied as the default during loss calculation to the network output. \n",
    "\n",
    "### Layers  \n",
    "The file `layer.py` contains implementations of neural network layers and regularization techniques that can be inserted as layers into the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten():\n",
    "    ''' Flatten layer used to reshape inputs into vector representation\n",
    "    \n",
    "    Layer should be used in the forward pass before a dense layer to \n",
    "    transform a given tensor into a vector. \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "\n",
    "    def forward(self, X):\n",
    "        ''' Reshapes a n-dim representation into a vector \n",
    "            by preserving the number of input rows.\n",
    "        \n",
    "        Examples:\n",
    "            [10000,[1,28,28]] -> [10000,784]\n",
    "        '''\n",
    "        self.X_shape = X.shape\n",
    "        self.out_shape = (self.X_shape[0], -1)    \n",
    "        out = X.reshape(-1).reshape(self.out_shape)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        ''' Restore dimensions before flattening operation\n",
    "        '''\n",
    "        out = dout.reshape(self.X_shape)\n",
    "        return out, []\n",
    "\n",
    "class FullyConnected():\n",
    "    ''' Fully connected layer implemtenting linear function hypothesis \n",
    "        in the forward pass and its derivation in the backward pass.\n",
    "    '''\n",
    "    def __init__(self, in_size, out_size):\n",
    "        ''' Initilize all learning parameters in the layer\n",
    "        \n",
    "        Weights will be initilized with modified Xavier initialization.\n",
    "        Biases will be initilized with zero. \n",
    "        '''\n",
    "        self.W = np.random.randn(in_size, out_size) * np.sqrt(2. / in_size)\n",
    "        self.b = np.zeros((1, out_size))\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        out = np.add(np.dot(self.X, self.W), self.b)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dX = np.dot(dout, self.W.T)\n",
    "        dW = np.dot(self.X.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "        return dX, [dW, db]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "The file `activation_func.py` contains implementations of activation functions you can use as a none linearity in your network. The functions work on the basis of matrix operations and not discret values, so that these can also be inserted as a layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    ''' Implements activation function rectified linear unit (ReLU) \n",
    "    \n",
    "    ReLU activation function is defined as the positive part of \n",
    "    its argument. Todo: insert arxiv paper reference\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "\n",
    "    def forward(self, X):\n",
    "        ''' In the forward pass return the identity for x < 0\n",
    "        \n",
    "        Safe input for backprop and forward all values that are above 0.\n",
    "        '''\n",
    "        self.X = X\n",
    "        return np.maximum(X, 0)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        ''' Derivative of ReLU\n",
    "        \n",
    "        Retruns:\n",
    "            dX: for all x \\elem X <= 0 in forward pass \n",
    "                return 0 else x\n",
    "            []: no gradients on ReLU operation\n",
    "        '''\n",
    "        dX = dout.copy()\n",
    "        dX[self.X <= 0] = 0\n",
    "        return dX, []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeuralNetwork class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    ''' Creates a neural network from a given layer architecture \n",
    "    \n",
    "    This class is suited for fully connected network and\n",
    "    convolutional neural network architectures. It connects \n",
    "    the layers and passes the data from one end to another.\n",
    "    '''\n",
    "    def __init__(self, layers, score_func=None):\n",
    "        ''' Setup a global parameter list and initilize a\n",
    "            score function that is used for predictions.\n",
    "        \n",
    "        Args:\n",
    "            layer: neural network architecture based on layer and activation function objects\n",
    "            score_func: function that is used as classifier on the output\n",
    "        '''\n",
    "        self.layers = layers\n",
    "        self.params = []\n",
    "        for layer in self.layers:\n",
    "            self.params.append(layer.params)\n",
    "        self.score_func = score_func\n",
    "\n",
    "    def forward(self, X):\n",
    "        ''' Pass input X through all layers in the network \n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "\n",
    "    def backward(self, dout):\n",
    "        grads = []\n",
    "        ''' Backprop through the network and keep a list of the gradients\n",
    "            from each layer.\n",
    "        '''\n",
    "        for layer in reversed(self.layers):\n",
    "            dout, grad = layer.backward(dout)\n",
    "            grads.append(grad)\n",
    "        return grads\n",
    "\n",
    "    def predict(self, X):\n",
    "        ''' Run a forward pass and use the score function to classify \n",
    "            the output.\n",
    "        '''\n",
    "        X = self.forward(X)\n",
    "        return np.argmax(self.score_func(X), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "Implementations of loss functions can be found in `loss_func.py`. A loss function object defines the criteria your network is evaluated during the optimization process and also contains score functions that can be used as classification criteria for predictions with the final model. Therefore it is necessary to create a loss function object and provide to the optimization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossCriteria():\n",
    "    ''' Implements diffrent typs of loss and score functions for neural networks\n",
    "    \n",
    "    Todo:\n",
    "        - Implement init that defines score and loss function \n",
    "    '''\n",
    "    def softmax(X):\n",
    "        ''' Numeric stable calculation of softmax\n",
    "        '''\n",
    "        exp_X = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
    "        return exp_X / np.sum(exp_X, axis=1, keepdims=True)\n",
    "\n",
    "    def cross_entropy_softmax(X, y):\n",
    "        ''' Computes loss and prepares dout for backprop \n",
    "\n",
    "        https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n",
    "        '''\n",
    "        m = y.shape[0]\n",
    "        p = LossCriteria.softmax(X)\n",
    "        log_likelihood = -np.log(p[range(m), y])\n",
    "        loss = np.sum(log_likelihood) / m\n",
    "        dout = p.copy()\n",
    "        dout[range(m), y] -= 1\n",
    "        return loss, dout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization with SGD\n",
    "The file `optimizer.py` contains implementations of optimization algorithms. Your optimizer needs your custom `network`, `data` and `loss function` and some additional hyperparameter as arguments to optimzie your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():   \n",
    "    def get_minibatches(X, y, batch_size):\n",
    "        ''' Decomposes data set into small subsets (batch)\n",
    "        '''\n",
    "        m = X.shape[0]\n",
    "        batches = []\n",
    "        for i in range(0, m, batch_size):\n",
    "            X_batch = X[i:i + batch_size, :, :, :]\n",
    "            y_batch = y[i:i + batch_size, ]\n",
    "            batches.append((X_batch, y_batch))\n",
    "        return batches    \n",
    "\n",
    "    def sgd(network, X_train, y_train, loss_function, batch_size=32, epoch=100, learning_rate=0.001, X_test=None, y_test=None, verbose=None):\n",
    "        ''' Optimize a given network with stochastic gradient descent \n",
    "        '''\n",
    "        minibatches = Optimizer.get_minibatches(X_train, y_train, batch_size)\n",
    "        for i in range(epoch):\n",
    "            loss = 0\n",
    "            if verbose:\n",
    "                print('Epoch',i + 1)\n",
    "            for X_mini, y_mini in minibatches:\n",
    "                # calculate loss and derivation of the last layer\n",
    "                loss, dout = loss_function(network.forward(X_mini), y_mini)\n",
    "                # calculate gradients via backpropagation\n",
    "                grads = network.backward(dout)\n",
    "                # run vanilla sgd update for all learnable parameters in self.params\n",
    "                for param, grad in zip(network.params, reversed(grads)):\n",
    "                    for i in range(len(grad)):\n",
    "                        param[i] += - learning_rate * grad[i]\n",
    "            if verbose:\n",
    "                train_acc = np.mean(y_train == network.predict(X_train))\n",
    "                test_acc = np.mean(y_test == network.predict(X_test))                                \n",
    "                print(\"Loss = {0} :: Training = {1} :: Test = {2}\".format(loss, train_acc, test_acc))\n",
    "        return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put it all together\n",
    "Now you have parts together to create and train a fully connected neural network. First, you have to define an individual network architecture by flatten the input and stacking fully connected layer with activation functions. Your custom architecture is given to a `NeuralNetwork` object that handles the inter-layer communication during the forward and backward pass. Finally, you optimize the model with a chosen algorithm, here stochastic gradient descent. That kind of pipeline is similar to the one you would create with a framework like Tensorflow or PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Loss = 0.18663092473713588 :: Training = 0.9699166666666666 :: Test = 0.9647\n",
      "Epoch 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-788b8c034e1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# optimize the network and a softmax loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mfcn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfcn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLossCriteria\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_softmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-08743555b4cb>\u001b[0m in \u001b[0;36msgd\u001b[0;34m(network, X_train, y_train, loss_function, batch_size, epoch, learning_rate, X_test, y_test, verbose)\u001b[0m\n\u001b[1;32m     29\u001b[0m                         \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss = {0} :: Training = {1} :: Test = {2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-e96a7dcd57e1>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mthe\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         '''\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-e96a7dcd57e1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     24\u001b[0m         '''\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-8caab0bec7ab>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# design a three hidden layer architecture with Dense-Layer\n",
    "# and ReLU as activation function\n",
    "def fcn_mnist():\n",
    "    flat = Flatten()\n",
    "    hidden_01 = FullyConnected(784, 500)\n",
    "    relu_01 = ReLU()\n",
    "    hidden_02 = FullyConnected(500, 200)\n",
    "    relu_02 = ReLU()\n",
    "    hidden_03 = FullyConnected(200, 100)\n",
    "    relu_03 = ReLU()\n",
    "    ouput = FullyConnected(100, 10)\n",
    "    return [flat, hidden_01, relu_01, hidden_02, relu_02, hidden_03, relu_03, ouput]\n",
    "\n",
    "# create a neural network on specified architecture with softmax as score function\n",
    "fcn = NeuralNetwork(fcn_mnist(), score_func=LossCriteria.softmax)\n",
    "\n",
    "# optimize the network and a softmax loss\n",
    "fcn = Optimizer.sgd(fcn, train_images, train_labels, LossCriteria.cross_entropy_softmax, batch_size=64, epoch=10, learning_rate=0.01, X_test=test_images, y_test=test_labels, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
